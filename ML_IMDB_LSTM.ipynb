{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdSpbeo3ID3gle9jfAKnfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LimaCondas/Machine_Learning_2021/blob/main/ML_IMDB_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqMFQW-xH5Fh"
      },
      "source": [
        "# 机器学习-工程作业\n",
        "\n",
        "**数据集**：IMDB数据集，包含来自互联网电影数据库（IMDB）的50000条严重两极分化的评论。数据集被分为用于训练的25000条评论和用于测试的25000条评论，训练集和测试集中都包括50%的正面评价和50%的负面评价。IMDB数据集内置于Keras库中，它已经过预处理，单词序列的评论已经被转换为整数序列，其中每个整数代表字典中的某个单词。\n",
        "\n",
        "\n",
        "**任务**：请在训练数据中选择前10000个最常出现的单词，并且在预训练的网络之上构建分类器。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfnMMbquIDlA"
      },
      "source": [
        "**数据集使用说明**\n",
        "Tensorflow\n",
        "* 读取命令\n",
        "from keras.datasets import imdb\n",
        "\n",
        "  (train_data, train_labels), (test_data, test_labels) = imdb.load_data()\n",
        "\n",
        "* imdb.load_data()函数说明：#https://blog.csdn.net/weixin_42272768/article/details/112093163 \n",
        "\n",
        "* Pytorch\n",
        "简便方法：使用上面命令下载好数据集后用相关命令转化为tensor数据类型即可。\n",
        "提醒：上面方法下载的数据集是已经处理好转化为整数序列，比较简便。另有torchtext导入的数据集为原始字符串还需要经过分词，建立词典，词向量嵌入等工作，较为复杂，有兴趣做NLP的同学可以自己探索，多去了解。\n",
        "原始数据集下载\n",
        "想看原始评论数据集（字符串形式）可以去下述网站下载数据包。\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eocg8lAuJwvn"
      },
      "source": [
        "## 代码"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 导入库，定义常量以及加载IMDB数据"
      ],
      "metadata": {
        "id": "1sXBcC0Gh4z-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IshM7--zJ05-"
      },
      "source": [
        "导入库，调用 Keras 库导入 IMDB 数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZU8e6h_IIyF"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "设定参数：词汇表大小、句子最大长度、批处理量、嵌入层层数、隐藏层层数、设备"
      ],
      "metadata": {
        "id": "kQRCmLqEOZ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 1000   #词汇表大小\n",
        "MAX_LEN = 200       #句子最大长度\n",
        "BATCH_SIZE = 256    #批处理量\n",
        "EMB_SIZE = 128      #嵌入层层数\n",
        "HID_SIZE = 128      #隐藏层层数\n",
        "DROPOUT = 0.2\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "IEK91fu6Ofzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "加载IMDB数据，TensorDataset --> RandomSampler --> DataLoader"
      ],
      "metadata": {
        "id": "qY35_UuHO27S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_WORDS) # 加载数据\n",
        "# 将训练集、测试集中的文本进行预处理，变成相同长度的文本，这里采用的规则是，在句子后面填充或截断\n",
        "x_train = pad_sequences(x_train, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "print(x_train.shape, x_test.shape)\n",
        "\n",
        "train_data = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
        "test_data = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))\n",
        "\n",
        "train_sampler = RandomSampler(train_data) # 从一个打乱的数据集进行采样\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE) # 将数据打包起来（一个batch_size是一组）\n",
        "\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "LLyY-SlAPehj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fdefd7f-b137-4ec7-f97b-4bdc04e0148c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 200) (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义模型\n",
        "\n",
        "* 使用LSTM模型进行文本分类，模型类中需要初始化函数、前向函数。\n",
        "\n",
        "* 初始化函数负责初始化模型的参数（词汇表大小、批处理量、嵌入层层数、隐藏层层数）以及模型的架构（本实验使用的是LSTM+线性层1+线性层2）。\n",
        "\n",
        "* 前向函数负责输出最终分类结果（本实验通过 Embedding --> dropout --> LSTM --> dropout --> fc1 --> relu --> avg_pool2d --> fc2 最终得到二分类结果）"
      ],
      "metadata": {
        "id": "S3OxW15Lh1bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, max_words, emb_size, hid_size, dropout):\n",
        "        super(Model, self).__init__()\n",
        "        self.max_words = max_words\n",
        "        self.emb_size = emb_size\n",
        "        self.hid_size = hid_size\n",
        "        self.dropout = dropout\n",
        "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
        "        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n",
        "                            batch_first=True, bidirectional=True) # 两层双向LSTM\n",
        "        self.dp = nn.Dropout(self.dropout)\n",
        "        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n",
        "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Embedding(x) # x.shape [batch_size, max_len, emb_size]\n",
        "        x = self.dp(x)\n",
        "        x, _ = self.LSTM(x) # [batch_size, max_len, hid_size*2]\n",
        "        x = self.dp(x)\n",
        "        x = F.relu(self.fc1(x)) # [batch_size, max_len, hid_size]\n",
        "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze() # [batch_size, 1, hid_size] -> [batch_size, hid_size]\n",
        "        out = self.fc2(x) # [batch_size, 2]\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "zqkW45tLiUUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义训练函数\n",
        "* 训练函数中首先定义损失函数（本实验使用交叉熵），然后加载数据，并通过将数据输入到模型中产生分类结果（y_），将损失计算后，通过反向传播（loss.backward()）更新模型参数（需要加上optimizer.step()模型参数才会被更新）"
      ],
      "metadata": {
        "id": "luvnc2CWigxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # 当有`Dropout`, `BatchNorm`时，需要加上这条\n",
        "    criterion = nn.CrossEntropyLoss() # 交叉熵\n",
        "    for batch_idx, (x,y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device) # x 是一个二维tensor,其中每一行是一个句子，一组batch_size个句子\n",
        "        y_ = model(x)\n",
        "        loss = criterion(y_, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if(batch_idx + 1)%10 == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()\n",
        "            ))"
      ],
      "metadata": {
        "id": "syb2pSgPim-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义测试函数\n",
        "测试函数与训练函数类似，而测试函数中需要计算准确率。"
      ],
      "metadata": {
        "id": "ATd3HC9JiqXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    test_loss = 0.0\n",
        "    acc = 0\n",
        "    for batch_idx, (x, y ) in enumerate(test_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # torch.no_grad()，对tensor的操作正常进行，但是track不被记录，无法求其梯度\n",
        "        with torch.no_grad():\n",
        "            y_ = model(x)\n",
        "        test_loss += criterion(y_, y)\n",
        "        pred = y_.max(-1, keepdim=True)[1] # .max()的输出为最大值和最大值的index， 获取index\n",
        "        acc += pred.eq(y.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
        "        test_loss, acc, len(test_loader.dataset),\n",
        "        100. * acc/len(test_loader.dataset)\n",
        "    ))\n",
        "    return acc/ len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "bvV6rNRUi2KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练，保存模型"
      ],
      "metadata": {
        "id": "ASv8F-71i4Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 初始化模型，定义优化函数（本实验使用的Adam优化器），定义模型保存路径"
      ],
      "metadata": {
        "id": "rXoE7uu2i-KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "best_acc = 0.0\n",
        "PATH = './model.pth' # 模型保存路径\n",
        "\n",
        "# 训练、测试以及保存模型\n",
        "for epoch in range(1, 5):\n",
        "    train(model,DEVICE,train_loader,optimizer,epoch)\n",
        "    acc = test(model,DEVICE,test_loader)\n",
        "    if best_acc < acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "    print(\"acc is {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKN7TZFii8XY",
        "outputId": "6fcf94d7-891c-4538-b697-68ee9bc6bb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (Embedding): Embedding(1000, 128)\n",
            "  (LSTM): LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (dp): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "Train Epoch: 1 [2304/25000 (9%)]\tLoss: 0.690520\n",
            "Train Epoch: 1 [4864/25000 (19%)]\tLoss: 0.678529\n",
            "Train Epoch: 1 [7424/25000 (30%)]\tLoss: 0.724879\n",
            "Train Epoch: 1 [9984/25000 (40%)]\tLoss: 0.687582\n",
            "Train Epoch: 1 [12544/25000 (50%)]\tLoss: 0.692820\n",
            "Train Epoch: 1 [15104/25000 (60%)]\tLoss: 0.690288\n",
            "Train Epoch: 1 [17664/25000 (70%)]\tLoss: 0.694465\n",
            "Train Epoch: 1 [20224/25000 (81%)]\tLoss: 0.764854\n",
            "Train Epoch: 1 [22784/25000 (91%)]\tLoss: 0.888429\n",
            "\n",
            "Test set: Average loss: 0.6779, Accuracy: 12944/25000 (52%)\n",
            "acc is 0.5178, best acc is 0.5178\n",
            "\n",
            "Train Epoch: 2 [2304/25000 (9%)]\tLoss: 0.718900\n",
            "Train Epoch: 2 [4864/25000 (19%)]\tLoss: 0.832925\n",
            "Train Epoch: 2 [7424/25000 (30%)]\tLoss: 0.695267\n",
            "Train Epoch: 2 [9984/25000 (40%)]\tLoss: 0.707344\n",
            "Train Epoch: 2 [12544/25000 (50%)]\tLoss: 0.671197\n",
            "Train Epoch: 2 [15104/25000 (60%)]\tLoss: 0.685370\n",
            "Train Epoch: 2 [17664/25000 (70%)]\tLoss: 0.702615\n",
            "Train Epoch: 2 [20224/25000 (81%)]\tLoss: 0.864072\n",
            "Train Epoch: 2 [22784/25000 (91%)]\tLoss: 0.647182\n",
            "\n",
            "Test set: Average loss: 0.9023, Accuracy: 12519/25000 (50%)\n",
            "acc is 0.5008, best acc is 0.5178\n",
            "\n",
            "Train Epoch: 3 [2304/25000 (9%)]\tLoss: 1.073909\n",
            "Train Epoch: 3 [4864/25000 (19%)]\tLoss: 0.772448\n",
            "Train Epoch: 3 [7424/25000 (30%)]\tLoss: 0.688971\n",
            "Train Epoch: 3 [9984/25000 (40%)]\tLoss: 0.735547\n",
            "Train Epoch: 3 [12544/25000 (50%)]\tLoss: 0.796756\n",
            "Train Epoch: 3 [15104/25000 (60%)]\tLoss: 0.776713\n",
            "Train Epoch: 3 [17664/25000 (70%)]\tLoss: 0.722699\n",
            "Train Epoch: 3 [20224/25000 (81%)]\tLoss: 0.707563\n",
            "Train Epoch: 3 [22784/25000 (91%)]\tLoss: 0.694330\n",
            "\n",
            "Test set: Average loss: 0.6820, Accuracy: 12500/25000 (50%)\n",
            "acc is 0.5000, best acc is 0.5178\n",
            "\n",
            "Train Epoch: 4 [2304/25000 (9%)]\tLoss: 0.692481\n",
            "Train Epoch: 4 [4864/25000 (19%)]\tLoss: 0.653711\n",
            "Train Epoch: 4 [7424/25000 (30%)]\tLoss: 0.646037\n",
            "Train Epoch: 4 [9984/25000 (40%)]\tLoss: 0.662275\n",
            "Train Epoch: 4 [12544/25000 (50%)]\tLoss: 0.671344\n",
            "Train Epoch: 4 [15104/25000 (60%)]\tLoss: 0.692113\n",
            "Train Epoch: 4 [17664/25000 (70%)]\tLoss: 0.693432\n",
            "Train Epoch: 4 [20224/25000 (81%)]\tLoss: 0.664172\n",
            "Train Epoch: 4 [22784/25000 (91%)]\tLoss: 0.626710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加载，测试模型\n",
        "加载保存的模型，调用测试函数进行测试"
      ],
      "metadata": {
        "id": "pbs_FyrLjDga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT)\n",
        "best_model.load_state_dict(torch.load(PATH))\n",
        "test(best_model, DEVICE, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2TEajNjG20",
        "outputId": "829f9405-dfe6-4f99-f057-ecf0c3739f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6779, Accuracy: 12944/25000 (52%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51776"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}